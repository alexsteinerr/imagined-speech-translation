INFO:__main__:Using device: cuda
INFO:src.data.dataset:Tokenizer vocabulary size: 51271
INFO:src.data.dataset:Frontal region: 16 channels - ['FC5', 'F5', 'F7', 'F3', 'FC1', 'F1', 'AF3', 'Fz', 'FC2', 'F2', 'AF4', 'Fp2', 'F4', 'F6', 'F8', 'FC6']
INFO:src.data.dataset:Temporal region: 9 channels - ['T9', 'FT9', 'T7', 'TP7', 'FT8', 'T10', 'FT10', 'T8', 'TP8']
INFO:src.data.dataset:Central region: 11 channels - ['C5', 'C3', 'FC3', 'C1', 'CP1', 'Cz', 'CP2', 'C2', 'C4', 'FC4', 'C6']
INFO:src.data.dataset:Parietal region: 16 channels - ['P7', 'PO9', 'P5', 'CP3', 'P3', 'PO3', 'PO1', 'POz', 'Oz', 'PO2', 'P4', 'PO4', 'PO10', 'P6', 'CP4', 'P8']
INFO:src.data.dataset:Total channels mapped: 52/125
INFO:src.data.dataset:Using existing tokenizer vocabulary without modifications
INFO:src.data.dataset:Tokenizer validation passed. Key IDs: pad=0, eos=104, bos=101
Setting up enhanced data loading...
INFO:src.data.dataset:Found 225 valid data files out of 225 total
INFO:src.data.dataset:Indexed 29466 valid samples from 225 files
INFO:src.data.dataset:Setup conservative text preprocessing patterns
INFO:src.data.dataset:Precomputing scalers on 500 samples...
INFO:src.data.dataset:Fitted scaler for frontal region with 825500 samples
INFO:src.data.dataset:Fitted scaler for temporal region with 825500 samples
INFO:src.data.dataset:Fitted scaler for central region with 825500 samples
INFO:src.data.dataset:Fitted scaler for parietal region with 825500 samples
INFO:src.data.dataset:Finished precomputing scalers on 500 successful samples
INFO:src.data.dataset:Computing dataset statistics...
INFO:src.data.dataset:Dataset stats computed: {'total_samples': 29466, 'valid_samples': 1000, 'avg_text_length': 8.665, 'text_length_std': 1.968444817616181, 'unique_eeg_shapes': 1, 'most_common_eeg_shape': (1, 125, 1651), 'region_channel_counts': {'frontal': 16, 'temporal': 9, 'central': 11, 'parietal': 16}}
INFO:__main__:Dataset loaded: 29466 samples
INFO:__main__:Region channel counts: {'frontal': 16, 'temporal': 9, 'central': 11, 'parietal': 16}
C:\Users\Marco\anaconda3\Lib\site-packages\torch\nn\modules\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
INFO:__main__:Data splits - Train: 23572, Val: 2946, Test: 2948
C:\Users\Marco\anaconda3\Lib\site-packages\transformers\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:absl:Using default tokenizer.
INFO:src.training.trainer:Starting training for 100 epochs...
Training Epoch 1:   0%|                                                                                                                                                                | 0/5893 [00:00<?, ?it/s]C:\Users\Marco\anaconda3\Lib\site-packages\torch\nn\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Training Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5893/5893 [1:01:14<00:00,  1.60it/s, loss=2.9614, lr=3.7e-04]
Building prefix dict from the default dictionary ...                                                                                                                                                            
DEBUG:jieba:Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\Marco\AppData\Local\Temp\jieba.cache
DEBUG:jieba:Loading model from cache C:\Users\Marco\AppData\Local\Temp\jieba.cache
Loading model cost 0.406 seconds.
DEBUG:jieba:Loading model cost 0.406 seconds.
Prefix dict has been built successfully.
DEBUG:jieba:Prefix dict has been built successfully.
Training Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5893/5893 [1:01:04<00:00,  1.61it/s, loss=1.7338, lr=5.0e-04]
Training Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5893/5893 [1:01:14<00:00,  1.60it/s, loss=1.1218, lr=4.9e-04]
INFO:src.training.trainer:Saved checkpoint: ./checkpoints/checkpoint_epoch_3.pth                                                                                                                                
Training Epoch 4:  63%|█████████████████████████████████████████████████████████████████████████████▉                                              | 3701/5893 [38:28<24:10,  1.51it/s, loss=1.3295, lr=4.9e-04]
