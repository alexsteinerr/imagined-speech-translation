2025-08-10 23:51:52,925 - __main__ - INFO - W&B initialized: EEG-Chinese-EMERGENCY-ANTICOLAPSE
2025-08-10 23:51:52,925 - __main__ - INFO - Loading tokenizer...
2025-08-10 23:51:53,276 - __main__ - INFO - Tokenizer loaded. Vocab size: 51271
2025-08-10 23:51:53,277 - __main__ - INFO - Creating model...
C:\Users\Marco\anaconda3\Lib\site-packages\torch\nn\modules\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
2025-08-10 23:51:55,903 - __main__ - INFO - Custom weights initialized
2025-08-10 23:51:55,905 - __main__ - INFO - Model created. Total params: 366,039,177, Trainable: 366,039,177
2025-08-10 23:51:55,906 - __main__ - INFO - Loading dataset...
2025-08-10 23:51:55,916 - src.data.dataset - INFO - Tokenizer vocabulary size: 51271
2025-08-10 23:51:55,923 - src.data.dataset - INFO - Frontal region: 16 channels - ['FC5', 'F5', 'F7', 'F3', 'FC1', 'F1', 'AF3', 'Fz', 'FC2', 'F2', 'AF4', 'Fp2', 'F4', 'F6', 'F8', 'FC6']
2025-08-10 23:51:55,923 - src.data.dataset - INFO - Temporal region: 9 channels - ['T9', 'FT9', 'T7', 'TP7', 'FT8', 'T10', 'FT10', 'T8', 'TP8']
2025-08-10 23:51:55,923 - src.data.dataset - INFO - Central region: 11 channels - ['C5', 'C3', 'FC3', 'C1', 'CP1', 'Cz', 'CP2', 'C2', 'C4', 'FC4', 'C6']
2025-08-10 23:51:55,924 - src.data.dataset - INFO - Parietal region: 12 channels - ['P7', 'P5', 'CP3', 'P3', 'PO3', 'PO1', 'PO2', 'P4', 'PO4', 'P6', 'CP4', 'P8']
2025-08-10 23:51:55,924 - src.data.dataset - INFO - Total channels mapped: 48/125
2025-08-10 23:51:55,925 - src.data.dataset - INFO - Tokenizer validation passed. Key IDs: pad=0, eos=104, bos=101
Discovering data files...
2025-08-10 23:51:55,926 - src.data.dataset - INFO - Found 225 .pkl files
2025-08-10 23:52:21,189 - src.data.dataset - INFO - Built index for 29466 samples
Computing normalization parameters from sample...
2025-08-10 23:52:31,581 - src.data.dataset - INFO - Fitted scaler for frontal with 100 samples
2025-08-10 23:52:31,600 - src.data.dataset - INFO - Fitted scaler for temporal with 100 samples
2025-08-10 23:52:31,623 - src.data.dataset - INFO - Fitted scaler for central with 100 samples
2025-08-10 23:52:31,659 - src.data.dataset - INFO - Fitted scaler for parietal with 100 samples
Dataset initialized with 29466 samples
2025-08-10 23:52:31,776 - __main__ - INFO - Dataset loaded: 29466 samples
2025-08-10 23:52:31,776 - __main__ - INFO - Region channel counts: {'frontal': 16, 'temporal': 9, 'central': 11, 'parietal': 12}
2025-08-10 23:52:31,780 - __main__ - INFO - Data splits - Train: 23572, Val: 2946, Test: 2948
2025-08-10 23:52:31,781 - __main__ - INFO - DataLoader settings - pin_memory: False, num_workers: 0
2025-08-10 23:52:31,781 - __main__ - INFO - Param group 0: 210,801,736 params, lr=1.00e-05
2025-08-10 23:52:31,781 - __main__ - INFO - Param group 1: 4,730,112 params, lr=1.00e-05
2025-08-10 23:52:31,783 - __main__ - INFO - Param group 2: 100,897,601 params, lr=1.00e-06
2025-08-10 23:52:31,783 - __main__ - INFO - Param group 3: 49,609,728 params, lr=1.00e-05
C:\Users\Marco\anaconda3\Lib\site-packages\transformers\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-08-10 23:52:31,786 - __main__ - INFO - Training steps: 73600 total, 100 warmup
2025-08-10 23:52:31,787 - __main__ - INFO - Optimizer and cosine scheduler created
2025-08-10 23:52:31,787 - __main__ - INFO - Checkpoints will be saved to: ./checkpoints/
2025-08-10 23:52:31,787 - absl - INFO - Using default tokenizer.
2025-08-10 23:52:31,816 - src.training.losses - INFO - Selected 1000 BoW indices from vocabulary of size 51271
2025-08-10 23:52:31,884 - src.training.trainer - INFO - Composite loss initialized with 1000 BoW indices
2025-08-10 23:52:31,884 - src.training.trainer - INFO - Trainer initialized with composite loss
2025-08-10 23:52:31,884 - src.training.trainer - INFO - Loss weights: {'ce': 0.2, 'align': 3.0, 'bow': 2.0, 'div': 5.0, 'var': 2.0}
2025-08-10 23:52:31,885 - __main__ - INFO - ============================================================
2025-08-10 23:52:31,885 - __main__ - INFO - Starting training with composite loss
2025-08-10 23:52:31,885 - __main__ - INFO - Epochs: 100
2025-08-10 23:52:31,885 - __main__ - INFO - Batch size: 4
2025-08-10 23:52:31,885 - __main__ - INFO - Accumulation steps: 8
2025-08-10 23:52:31,885 - __main__ - INFO - Effective batch size: 32
2025-08-10 23:52:31,886 - __main__ - INFO - ============================================================
2025-08-10 23:52:31,886 - src.training.trainer - INFO - ============================================================
2025-08-10 23:52:31,886 - src.training.trainer - INFO - Starting training with composite loss
2025-08-10 23:52:31,886 - src.training.trainer - INFO - Initial loss weights: {'ce': 0.2, 'align': 3.0, 'bow': 2.0, 'div': 5.0, 'var': 2.0}
2025-08-10 23:52:31,886 - src.training.trainer - INFO - Adaptive scheduling: True
2025-08-10 23:52:31,886 - src.training.trainer - INFO - ============================================================
Training Epoch 1:   0%|                                                                                                                                                                | 0/5893 [00:00<?, ?it/s]C:\Users\Marco\anaconda3\Lib\site-packages\torch\nn\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
Training Epoch 1: 100%|████████████████████████████████████████████████████████████████████| 5893/5893 [2:15:01<00:00,  1.37s/it, loss=4.7233, lr=1.0e-05, ce=7.471, ali=0.341, bow=0.555, div=0.211, var=0.020]
2025-08-11 02:07:33,980 - src.training.trainer - INFO - Epoch 1 - Avg Loss: 5.4595
2025-08-11 02:07:33,981 - src.training.trainer - INFO - Loss components: {'loss_ce': 7.5824152372616895, 'loss_align': 0.1827450331682694, 'loss_bow': 0.6496032643415275, 'loss_div': 0.2329791338933196, 'loss_var': 0.4653289546053692}
Building prefix dict from the default dictionary ...                                                                                                                                                            
2025-08-11 02:25:55,793 - jieba - DEBUG - Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\Marco\AppData\Local\Temp\jieba.cache
2025-08-11 02:25:55,793 - jieba - DEBUG - Loading model from cache C:\Users\Marco\AppData\Local\Temp\jieba.cache
Loading model cost 0.444 seconds.
2025-08-11 02:25:56,238 - jieba - DEBUG - Loading model cost 0.444 seconds.
Prefix dict has been built successfully.
2025-08-11 02:25:56,239 - jieba - DEBUG - Prefix dict has been built successfully.
2025-08-11 02:25:58,618 - src.training.trainer - INFO - Validation - BLEU-4: 0.000, Diversity: 1.000, Loss: 6.6776
Training Epoch 2: 100%|████████████████████████████████████████████████████████████████████| 5893/5893 [2:15:40<00:00,  1.38s/it, loss=2.8218, lr=1.0e-05, ce=6.840, ali=0.013, bow=0.620, div=0.033, var=0.005]
2025-08-11 04:41:38,871 - src.training.trainer - INFO - Epoch 2 - Avg Loss: 3.5687
2025-08-11 04:41:38,872 - src.training.trainer - INFO - Loss components: {'loss_ce': 7.144221957219835, 'loss_align': 0.04391448618320895, 'loss_bow': 0.6212841826830051, 'loss_div': 0.08521291548909325, 'loss_var': 0.16971827760928979}
2025-08-11 05:00:09,780 - src.training.trainer - INFO - Validation - BLEU-4: 0.000, Diversity: 1.000, Loss: 7.1236                                                                                              
2025-08-11 05:00:17,924 - src.training.trainer - INFO - Checkpoint saved: ./checkpoints/checkpoint_epoch_2.pth
Training Epoch 3: 100%|████████████████████████████████████████████████████████████████████| 5893/5893 [2:15:45<00:00,  1.38s/it, loss=4.0335, lr=1.0e-05, ce=5.002, ali=0.013, bow=0.713, div=0.150, var=0.408]
2025-08-11 07:16:03,201 - src.training.trainer - INFO - Epoch 3 - Avg Loss: 3.4101
2025-08-11 07:16:03,202 - src.training.trainer - INFO - Loss components: {'loss_ce': 5.7455226966000845, 'loss_align': 0.04325569257390434, 'loss_bow': 0.6502966066784797, 'loss_div': 0.08310426390395909, 'loss_var': 0.20753772623553157}
2025-08-11 07:34:36,041 - src.training.trainer - INFO - Updated loss weights: {'ce': 0.2, 'align': 3.0, 'bow': 2.0, 'div': 4.5, 'var': 2.0}                                                                     
2025-08-11 07:34:36,293 - src.training.trainer - INFO - Validation - BLEU-4: 0.000, Diversity: 1.000, Loss: 7.1906
Training Epoch 4: 100%|████████████████████████████████████████████████████████████████████| 5893/5893 [2:15:36<00:00,  1.38s/it, loss=2.7229, lr=1.0e-05, ce=4.623, ali=0.002, bow=0.731, div=0.033, var=0.090]
2025-08-11 09:50:13,192 - src.training.trainer - INFO - Epoch 4 - Avg Loss: 3.2359
2025-08-11 09:50:13,192 - src.training.trainer - INFO - Loss components: {'loss_ce': 4.86119139107504, 'loss_align': 0.030268700779092873, 'loss_bow': 0.719247412614824, 'loss_div': 0.07423141887280303, 'loss_var': 0.20015884716109847}
2025-08-11 10:08:36,233 - src.training.trainer - INFO - Updated loss weights: {'ce': 0.2, 'align': 3.0, 'bow': 2.0, 'div': 4.05, 'var': 2.0}                                                                    
2025-08-11 10:08:36,469 - src.training.trainer - INFO - Validation - BLEU-4: 0.000, Diversity: 1.000, Loss: 7.3484
2025-08-11 10:08:49,252 - src.training.trainer - INFO - Checkpoint saved: ./checkpoints/checkpoint_epoch_4.pth
Training Epoch 5: 100%|████████████████████████████████████████████████████████████████████| 5893/5893 [2:15:20<00:00,  1.38s/it, loss=2.7913, lr=9.9e-06, ce=4.669, ali=0.050, bow=0.675, div=0.060, var=0.058]
2025-08-11 12:24:09,728 - src.training.trainer - INFO - Epoch 5 - Avg Loss: 2.9529
2025-08-11 12:24:09,729 - src.training.trainer - INFO - Loss components: {'loss_ce': 4.641436945738664, 'loss_align': 0.018733835492203633, 'loss_bow': 0.700478246003445, 'loss_div': 0.09272235664198465, 'loss_var': 0.09596769857456935}
2025-08-11 12:42:33,315 - src.training.trainer - INFO - Updated loss weights: {'ce': 0.2, 'align': 3.0, 'bow': 2.0, 'div': 3.645, 'var': 2.0}                                                                   
2025-08-11 12:42:33,564 - src.training.trainer - INFO - Validation - BLEU-4: 0.000, Diversity: 1.000, Loss: 5.6377
Training Epoch 6:  44%|██████████████████████████████                                      | 2601/5893 [59:36<1:20:36,  1.47s/it, loss=3.3308, lr=9.9e-06, ce=4.492, ali=0.063, bow=0.763, div=0.092, var=0.191]
